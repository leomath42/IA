{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "733f6717",
   "metadata": {},
   "source": [
    "# T3 de Inteligência Artificial\n",
    "\n",
    "Aluno: Leonardo Souza\n",
    "\n",
    "Professor: Eduardo Bezerra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a4492d",
   "metadata": {},
   "source": [
    "O objetivo deste trabalho é a implementação do algoritmo Q-Learning pelo método de aproximação linear.\n",
    "\n",
    "O algoritmo será testado dentro do ambiente Taxi-v3 do OpenGym, para tal, será necessário instalar algumas dependências:\n",
    "\n",
    "    pip3 install gym==0.17.3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808b6773",
   "metadata": {},
   "source": [
    "### Testando a criação do ambiente Taxi-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa0e1393",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+\n|R: | : :G|\n| : | : : |\n| : : : : |\n| | : | :\u001b[43m \u001b[0m|\n|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6\n114\n1\n14 -1 False {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "action = env.action_space.sample() # Seleciona ação a executar\n",
    "new_state, reward, done, info = env.step(action)\n",
    "print(env.action_space.n)\n",
    "\n",
    "print(state)\n",
    "print(action)\n",
    "print(new_state, reward, done, info)\n",
    "\n",
    "\n",
    "def feature_manhattan_distance_taxi_passenger(state, action):\n",
    "    s_  =  env.P[state][action][0][1]\n",
    "    \n",
    "\n",
    "    l, c, p, d = env.unwrapped.decode(s_)\n",
    "    p = env.unwrapped.locs[p]\n",
    "    \n",
    "    return abs(l - p[0]) + abs(c - p[1])\n",
    "\n",
    "\n",
    "def feature_manhattan_distance_taxi_destiny(state, action):\n",
    "    s_  =  env.P[state][action][0][1]\n",
    "    \n",
    "    l, c, p, d = env.unwrapped.decode(s_)\n",
    "    d = env.unwrapped.locs[d]\n",
    "\n",
    "    return abs(l - d[0]) + abs(c - d[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3 3 0 3\n3 2 3 0\n{0: [(1.0, 100, -1, False)], 1: [(1.0, 0, -1, False)], 2: [(1.0, 20, -1, False)], 3: [(1.0, 0, -1, False)], 4: [(1.0, 16, -1, False)], 5: [(1.0, 0, -10, False)]}\n[100, 0, 20, 0, 16, 0]\n"
     ]
    }
   ],
   "source": [
    "# env.s = 352\n",
    "# state = env.reset()\n",
    "# print(env)\n",
    "# print(env.s)\n",
    "# print(env.P[env.s])\n",
    "# # print(env.step(5))\n",
    "# env.s = 0\n",
    "# print(env)\n",
    "# print(env.s)\n",
    "# print(env.P[0])\n",
    "# # print(env.step(0))\n",
    "\n",
    "\n",
    "# print(env.action_space)\n",
    "# print(env.observation_space)\n",
    "l, c, p, d = env.unwrapped.decode(state)\n",
    "\n",
    "print(l, c, p, d)\n",
    "\n",
    "l, c, p, d = env.unwrapped.decode(new_state)\n",
    "\n",
    "print(l, c, p, d)\n",
    "print(env.P[0])\n",
    "\n",
    "neighbors = [ env.P[0][_][0][1] for _ in env.P[0]]\n",
    "print(neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.67955667 0.74524331]\n[5, 4]\n<class 'numpy.ndarray'>\n[3.39778335 2.98097324]\n6.378756591130507\n6.378756591130507\n"
     ]
    }
   ],
   "source": [
    "# print(feature_manhattan_distance_taxi_passenger(state, action))\n",
    "# print(feature_manhattan_distance_taxi_passenger(new_state, action))\n",
    "\n",
    "# a = np.array([feature_manhattan_distance_taxi_passenger, feature_manhattan_distance_taxi_destiny])\n",
    "b = np.random.rand(a.size)\n",
    "print(b)\n",
    "features = [f(state, action) for f in  [feature_manhattan_distance_taxi_passenger, feature_manhattan_distance_taxi_destiny]]\n",
    "# features = np.array([f(state, action) for f in  [feature_manhattan_distance_taxi_passenger, feature_manhattan_distance_taxi_destiny]])\n",
    "print(features)\n",
    "features = np.array([5, 4])\n",
    "print(type(features))\n",
    "\n",
    "print(features * b)\n",
    "print((features * b).sum())\n",
    "print( b @ features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6129721c",
   "metadata": {},
   "source": [
    "## Implementação do Q-Learning Linear"
   ]
  },
  {
   "source": [
    "### Definição dos hyperparâmetros"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Implementação das Classe QLearningLinear"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "189fe22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class QLearningLinear\n",
    "class QLearningLinear(object):\n",
    "    \n",
    "    def __init__(self, features:list , learning_rate = 0.7, gamma=0.9):\n",
    "        # cria um numpy array de pesos sendo 1:n, onde n representa o número de features\n",
    "        self.weigths = np.random.rand(len(features))\n",
    "        # covnerte a lista de features para um numpy array.\n",
    "        self.features = features\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def Q(self, state, action):\n",
    "        \"\"\"\n",
    "            Q(S,A) \n",
    "        \"\"\"\n",
    "\n",
    "        f_values = [f(state, action) for f in  self.features]\n",
    "        f_values = np.array(f_values)\n",
    "\n",
    "        return (self.weigths * f_values).sum()\n",
    "        \n",
    "    # def \n",
    "    def learn(self, state, action, reward, next_state, done, episode):\n",
    "        '''\n",
    "            alhpa == learning rate\n",
    "            gamma == decaiment\n",
    "\n",
    "            wi <= wi + alpha * [r + gamma * max_Q(s',a') - Q(s,a)]\n",
    "        '''\n",
    "\n",
    "        # neighbors = [ env.P[next_state][_][0][1] for _ in env.P[next_state]]\n",
    "        # max_q = [self.Q(v, None)  for v in neighbors ]\n",
    "        # max_q = max(max_q)\n",
    "        # neighbors = [ env.P[next_state][_][0][1] for _ in env.P[next_state]]\n",
    "        \n",
    "        max_q = [self.Q(next_state, action)  for action in range(6) ]\n",
    "        max_q = max(max_q)\n",
    "\n",
    "        q = self.Q(state, action)\n",
    "\n",
    "        self.weights = self.weigths + self.learning_rate * (reward + self.gamma * max_q - q)\n",
    "\n",
    "        return self.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "c00a3c8f",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "263\n"
     ]
    }
   ],
   "source": [
    "qll = QLearningLinear([feature_manhattan_distance_taxi_passenger, feature_manhattan_distance_taxi_destiny])\n",
    "state = env.reset()\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6\n263\n0\n363 -1 False {'prob': 1.0}\n[0.00440149 0.47171546]\n[0.2501162  0.71743017]\n"
     ]
    }
   ],
   "source": [
    "action  = 0\n",
    "new_state, reward, done, info = env.step(action)\n",
    "print(env.action_space.n)\n",
    "\n",
    "print(state)\n",
    "print(action)\n",
    "print(new_state, reward, done, info)\n",
    "\n",
    "\n",
    "print(qll.learn(state, action, reward, new_state, done, 0))\n",
    "print(qll.weigths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.1740869654921213\n3 1 3 2\n[(1.0, 434, -1, False)]\n"
     ]
    }
   ],
   "source": [
    "print(qll.Q(state, None))\n",
    "l, c, p, d = env.unwrapped.decode(state)\n",
    "\n",
    "print(l, c, p, d)\n",
    "\n",
    "print(env.P[state][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}