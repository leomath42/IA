{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "733f6717",
   "metadata": {},
   "source": [
    "# T3 de Inteligência Artificial\n",
    "\n",
    "Aluno: Leonardo Souza\n",
    "\n",
    "Professor: Eduardo Bezerra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a4492d",
   "metadata": {},
   "source": [
    "O objetivo deste trabalho é a implementação do algoritmo Q-Learning pelo método de aproximação linear.\n",
    "\n",
    "O algoritmo será testado dentro do ambiente Taxi-v3 do OpenGym, para tal, será necessário instalar algumas dependências:\n",
    "\n",
    "    pip3 install gym==0.17.3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808b6773",
   "metadata": {},
   "source": [
    "### Testando a criação do ambiente Taxi-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa0e1393",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+\n|\u001b[35m\u001b[43mR\u001b[0m\u001b[0m: | : :G|\n| : | : : |\n| : : : : |\n| | : | : |\n|\u001b[34;1mY\u001b[0m| : |B: |\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6\n201\n5\n201 -10 False {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "action = env.action_space.sample() # Seleciona ação a executar\n",
    "new_state, reward, done, info = env.step(action)\n",
    "print(env.action_space.n)\n",
    "\n",
    "print(state)\n",
    "print(action)\n",
    "print(new_state, reward, done, info)\n",
    "\n",
    "\n",
    "def feature_manhattan_distance_taxi_passenger(state, action):\n",
    "    s_  =  env.P[state][action][0][1]\n",
    "    \n",
    "\n",
    "    l, c, p, d = env.unwrapped.decode(s_)\n",
    "    \n",
    "    if p >= 4:\n",
    "        return 0\n",
    "\n",
    "    p = env.unwrapped.locs[p]\n",
    "    \n",
    "    return abs(l - p[0]) + abs(c - p[1])\n",
    "\n",
    "\n",
    "def feature_manhattan_distance_taxi_destiny(state, action):\n",
    "    s_  =  env.P[state][action][0][1]\n",
    "    \n",
    "    l, c, p, d = env.unwrapped.decode(s_)\n",
    "    d = env.unwrapped.locs[d]\n",
    "\n",
    "    return abs(l - d[0]) + abs(c - d[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2 0 2 0\n2 0 2 0\n{0: [(1.0, 100, -1, False)], 1: [(1.0, 0, -1, False)], 2: [(1.0, 20, -1, False)], 3: [(1.0, 0, -1, False)], 4: [(1.0, 16, -1, False)], 5: [(1.0, 0, -10, False)]}\n[100, 0, 20, 0, 16, 0]\n"
     ]
    }
   ],
   "source": [
    "# env.s = 352\n",
    "# state = env.reset()\n",
    "# print(env)\n",
    "# print(env.s)\n",
    "# print(env.P[env.s])\n",
    "# # print(env.step(5))\n",
    "# env.s = 0\n",
    "# print(env)\n",
    "# print(env.s)\n",
    "# print(env.P[0])\n",
    "# # print(env.step(0))\n",
    "\n",
    "\n",
    "# print(env.action_space)\n",
    "# print(env.observation_space)\n",
    "l, c, p, d = env.unwrapped.decode(state)\n",
    "\n",
    "print(l, c, p, d)\n",
    "\n",
    "l, c, p, d = env.unwrapped.decode(new_state)\n",
    "\n",
    "print(l, c, p, d)\n",
    "print(env.P[0])\n",
    "\n",
    "neighbors = [ env.P[0][_][0][1] for _ in env.P[0]]\n",
    "print(neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.67955667 0.74524331]\n[5, 4]\n<class 'numpy.ndarray'>\n[3.39778335 2.98097324]\n6.378756591130507\n6.378756591130507\n"
     ]
    }
   ],
   "source": [
    "# print(feature_manhattan_distance_taxi_passenger(state, action))\n",
    "# print(feature_manhattan_distance_taxi_passenger(new_state, action))\n",
    "\n",
    "# a = np.array([feature_manhattan_distance_taxi_passenger, feature_manhattan_distance_taxi_destiny])\n",
    "b = np.random.rand(a.size)\n",
    "print(b)\n",
    "features = [f(state, action) for f in  [feature_manhattan_distance_taxi_passenger, feature_manhattan_distance_taxi_destiny]]\n",
    "# features = np.array([f(state, action) for f in  [feature_manhattan_distance_taxi_passenger, feature_manhattan_distance_taxi_destiny]])\n",
    "print(features)\n",
    "features = np.array([5, 4])\n",
    "print(type(features))\n",
    "\n",
    "print(features * b)\n",
    "print((features * b).sum())\n",
    "print( b @ features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6129721c",
   "metadata": {},
   "source": [
    "## Implementação do Q-Learning Linear"
   ]
  },
  {
   "source": [
    "### Definição dos hyperparâmetros"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Implementação das Classe QLearningLinear"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "189fe22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class QLearningLinear\n",
    "class QLearningLinear(object):\n",
    "    \n",
    "    def __init__(self, features:list , learning_rate = 0.7, gamma=0.9):\n",
    "        # cria um numpy array de pesos sendo 1:n, onde n representa o número de features\n",
    "        self.weights  = np.random.rand(len(features))\n",
    "        # covnerte a lista de features para um numpy array.\n",
    "        self.features = features\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \n",
    "        max_q = max([(self.Q(state, action), action)  for action in range(6) ])\n",
    "\n",
    "        return max_q[1]\n",
    "\n",
    "    def Q(self, state, action):\n",
    "        \"\"\"\n",
    "            Q(S,A) \n",
    "        \"\"\"\n",
    "\n",
    "        f_values = [f(state, action) for f in  self.features]\n",
    "        f_values = np.array(f_values)\n",
    "\n",
    "        return (self.weights  * f_values).sum()\n",
    "        \n",
    "    # def \n",
    "    def learn(self, state, action, reward, next_state, done, episode):\n",
    "        '''\n",
    "            alhpa == learning rate\n",
    "            gamma == decaiment\n",
    "\n",
    "            wi <= wi + alpha * [r + gamma * max_Q(s',a') - Q(s,a)]\n",
    "        '''\n",
    "\n",
    "        # neighbors = [ env.P[next_state][_][0][1] for _ in env.P[next_state]]\n",
    "        # max_q = [self.Q(v, None)  for v in neighbors ]\n",
    "        # max_q = max(max_q)\n",
    "        # neighbors = [ env.P[next_state][_][0][1] for _ in env.P[next_state]]\n",
    "\n",
    "        max_q = [self.Q(next_state, action)  for action in range(6) ]\n",
    "        max_q = max(max_q)\n",
    "\n",
    "        q = self.Q(state, action)\n",
    "\n",
    "        self.weights  = self.weights  + self.learning_rate * (reward + self.gamma * max_q - q)\n",
    "\n",
    "        return self.weights \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c00a3c8f",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "qll = QLearningLinear([feature_manhattan_distance_taxi_passenger, feature_manhattan_distance_taxi_destiny], learning_rate=0.2)\n",
    "state = env.reset()\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action  = 0\n",
    "# new_state, reward, done, info = env.step(action)\n",
    "# print(env.action_space.n)\n",
    "\n",
    "# print(state)\n",
    "# print(action)\n",
    "# print(new_state, reward, done, info)\n",
    "\n",
    "\n",
    "# print(qll.learn(state, action, reward, new_state, done, 0))\n",
    "# print(qll.weigths)\n",
    "\n",
    "def train_fit(agent, env, epochs):\n",
    "    initial_state = env.reset()\n",
    "    state = initial_state\n",
    "    hist = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        action = agent.choose_action(state)\n",
    "        \n",
    "        # transição\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        # print(action)\n",
    "        weights  = agent.learn(state, action, reward, new_state, done, epoch)\n",
    "\n",
    "        state = new_state\n",
    "        \n",
    "        hist.append([state, action, weights ])\n",
    "        # episode_rewards.append(reward)\n",
    "        \n",
    "        if done == True:\n",
    "          break\n",
    "    \n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testando o histórico de mudanças\n",
    "\n",
    "history_states =  train_fit(qll, env, 10)\n",
    "for i in history_states:\n",
    "    print(i)\n",
    "    \n",
    "print(qll.choose_action(state))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[51, 5, array([-1.99031341, -1.34113318])]\n[151, 0, array([-1.28433103, -0.6351508 ])]\n[251, 0, array([-0.99796877, -0.34878854])]\n[351, 0, array([-0.88172278, -0.23254256])]\n[451, 0, array([-0.92495058, -0.27577035])]\n[431, 3, array([-1.09542076, -0.44624053])]\n[431, 5, array([-3.05566272, -2.40648249])]\n[431, 5, array([-4.89829017, -4.24910994])]\n[431, 5, array([-6.63035996, -5.98117974])]\n[431, 5, array([-8.25850558, -7.60932535])]\n0\n1\n0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "State id: 16\n0 0 4 0\n(0, 0)\n[(0, 0), (0, 4), (4, 0), (4, 3)]\n+---------+\n|\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: | : :G|\n| : | : : |\n| : : : : |\n| | : | : |\n|Y| : |B: |\n+---------+\n  (South)\n"
     ]
    }
   ],
   "source": [
    "# state = env.encode(0, 0, 4, 0)\n",
    "# print(\"State id:\", state)\n",
    "# env.s = state\n",
    "\n",
    "# l, c, p, d = env.unwrapped.decode(s_)\n",
    "# print(l, c, p, d)\n",
    "# print(env.unwrapped.locs[d])\n",
    "# print(env.unwrapped.locs)\n",
    "# env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+\n|\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: | : :G|\n| : | : : |\n| : : : : |\n| | : | : |\n|Y| : |B: |\n+---------+\n  (South)\n+---------+\n|\u001b[35mR\u001b[0m: | : :G|\n|\u001b[42m_\u001b[0m: | : : |\n| : : : : |\n| | : | : |\n|Y| : |B: |\n+---------+\n  (South)\n+---------+\n|\u001b[35mR\u001b[0m: | : :G|\n| : | : : |\n|\u001b[42m_\u001b[0m: : : : |\n| | : | : |\n|Y| : |B: |\n+---------+\n  (South)\n+---------+\n|\u001b[35mR\u001b[0m: | : :G|\n| : | : : |\n| : : : : |\n|\u001b[42m_\u001b[0m| : | : |\n|Y| : |B: |\n+---------+\n  (South)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    state = env.encode(, 0, 4, 0)\n",
    "    env.s = state\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}