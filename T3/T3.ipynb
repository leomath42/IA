{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "733f6717",
   "metadata": {},
   "source": [
    "# T3 de Inteligência Artificial\n",
    "\n",
    "Aluno: Leonardo Souza\n",
    "\n",
    "Professor: Eduardo Bezerra\n",
    "\n",
    "Link do vídeo: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a4492d",
   "metadata": {},
   "source": [
    "O objetivo deste trabalho é a implementação do algoritmo Q-Learning pelo método de aproximação linear.\n",
    "\n",
    "O algoritmo será testado dentro do ambiente Taxi-v3 do OpenGym, para tal, será necessário instalar algumas dependências:\n",
    "\n",
    "    pip3 install gym==0.17.3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808b6773",
   "metadata": {},
   "source": [
    "### Testando a criação do ambiente Taxi-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa0e1393",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+\n|\u001b[34;1mR\u001b[0m: | : :G|\n| : | : : |\n|\u001b[43m \u001b[0m: : : : |\n| | : | : |\n|\u001b[35mY\u001b[0m| : |B: |\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6129721c",
   "metadata": {},
   "source": [
    "## Implementação do Q-Learning Linear"
   ]
  },
  {
   "source": [
    "### Definição dos hyperparâmetros\n",
    "\n",
    "learning_rate = 0.7\n",
    "\n",
    "gamma = 0.618\n",
    "\n",
    "decay_rate = 0.0001\n",
    "\n",
    "features:\n",
    "\n",
    "    - feature_manhattan_distance_taxi_passenger\n",
    "    \n",
    "    - feature_manhattan_distance_taxi_destiny\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### features:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature_manhattan_distance_taxi_passenger(state, action):\n",
    "    s_  =  env.P[state][action][0][1]\n",
    "\n",
    "    l, c, p, d = env.unwrapped.decode(s_)\n",
    "\n",
    "    if p > 3:\n",
    "        return 0\n",
    "    \n",
    "    p = env.unwrapped.locs[p]\n",
    "    v = (abs(l - p[0]) + abs(c - p[1]))\n",
    "\n",
    "    if v == 0:\n",
    "        return 0\n",
    "\n",
    "    return 1/v\n",
    "\n",
    "\n",
    "def feature_manhattan_distance_taxi_destiny(state, action):\n",
    "    s_  =  env.P[state][action][0][1]\n",
    "    \n",
    "    l, c, p, d = env.unwrapped.decode(s_)\n",
    "    \n",
    "    if p < 4:\n",
    "        return 0\n",
    "    \n",
    "    d = env.unwrapped.locs[d]\n",
    "    v = (abs(l - d[0]) + abs(c - d[1]))\n",
    "\n",
    "    if v == 0:\n",
    "        return 0\n",
    "\n",
    "    return 1/v\n"
   ]
  },
  {
   "source": [
    "### Implementação das Classe QLearningLinear"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "189fe22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class QLearningLinear\n",
    "class QLearningLinear(object):\n",
    "    \n",
    "    def __init__(self, features:list , n_actions=6, learning_rate = 0.7, gamma=0.618, decay_rate=0.0001):\n",
    "        # cria um numpy array de pesos sendo 1:n, onde n representa o número de features\n",
    "        self.weights  = np.random.rand(len(features))\n",
    "\n",
    "        self.features = features\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.max_epsilon = 1.0\n",
    "        self.min_epsilon = 0.01\n",
    "        self.epsilon = 1.0\n",
    "        self.n_actions = n_actions\n",
    "        self.decay_rate = decay_rate\n",
    "\n",
    "    def choose_action(self, state, explore=True):\n",
    "        exploration_tradeoff = np.random.uniform(0, 1)\n",
    "    \n",
    "        if explore and exploration_tradeoff < self.epsilon:\n",
    "            return np.random.randint(self.n_actions) \n",
    "\n",
    "        max_q = round(max([self.Q(state, action) for action in range(6) ]))\n",
    "\n",
    "        if 0 <= max_q  <= 5:\n",
    "            return max_q\n",
    "        else:\n",
    "            return np.random.randint(self.n_actions) \n",
    "\n",
    "    def Q(self, state, action):\n",
    "        \"\"\"\n",
    "            Q(S,A) \n",
    "        \"\"\"\n",
    "\n",
    "        f_values = [f(state, action) for f in  self.features]\n",
    "        f_values = np.array(f_values)\n",
    "\n",
    "        return (self.weights  * f_values).sum()\n",
    "        \n",
    "    def learn(self, state, action, reward, next_state, done, episode):\n",
    "        '''\n",
    "            alhpa == learning rate\n",
    "            gamma == decaiment\n",
    "\n",
    "            wi <= wi + alpha * [r + gamma * max_Q(s',a') - Q(s,a)] * fi(s, a)\n",
    "        '''\n",
    "\n",
    "        # neighbors = [ env.P[next_state][_][0][1] for _ in env.P[next_state]]\n",
    "        # max_q = [self.Q(v, None)  for v in neighbors ]\n",
    "        # max_q = max(max_q)\n",
    "        # neighbors = [ env.P[next_state][_][0][1] for _ in env.P[next_state]]\n",
    "\n",
    "        max_q = [self.Q(next_state, action)  for action in range(6) ]\n",
    "        max_q = max(max_q)\n",
    "\n",
    "        q = self.Q(state, action)\n",
    "        difference = (reward + self.gamma * max_q - q)\n",
    "        \n",
    "        fi_ = np.array([f(state, action) for f in  self.features])\n",
    "\n",
    "        self.weights  = self.weights  + self.learning_rate * difference * fi_\n",
    "\n",
    "        if done:\n",
    "            self.epsilon = self.min_epsilon + (self.max_epsilon - self.min_epsilon) * \\\n",
    "            np.exp(-self.decay_rate * episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "c00a3c8f",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<__main__.QLearningLinear object at 0x7f6bbf14c880> 94\n"
     ]
    }
   ],
   "source": [
    "qll = QLearningLinear([feature_manhattan_distance_taxi_passenger, feature_manhattan_distance_taxi_destiny], learning_rate=0.7)\n",
    "state = env.reset()\n",
    "print(qll, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_fit(agent, env, epoch, debug=True):\n",
    "    initial_state = env.reset()\n",
    "    state = initial_state\n",
    "    hist = []\n",
    "    rewards = []\n",
    "\n",
    "    while True:\n",
    "        action = agent.choose_action(state)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        agent.learn(state, action, reward, new_state, done, epoch)\n",
    "\n",
    "        state = new_state\n",
    "        \n",
    "        # hist.append([state, action, weights ])\n",
    "        rewards.append(reward)\n",
    "\n",
    "        if done == True:\n",
    "          break\n",
    "      \n",
    "    mean = np.array(reward).mean()\n",
    "    \n",
    "    if debug:\n",
    "      print(\"epoch {} reward mean: {}\".format(epoch, mean))\n",
    "\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 reward mean: 20.0\n",
      "epoch 1 reward mean: 20.0\n",
      "epoch 2 reward mean: 20.0\n",
      "epoch 3 reward mean: 20.0\n",
      "epoch 4 reward mean: 20.0\n",
      "epoch 5 reward mean: 20.0\n",
      "epoch 6 reward mean: 20.0\n",
      "epoch 7 reward mean: 20.0\n",
      "epoch 8 reward mean: 20.0\n",
      "epoch 9 reward mean: 20.0\n",
      "epoch 10 reward mean: 20.0\n",
      "epoch 11 reward mean: 20.0\n",
      "epoch 12 reward mean: 20.0\n",
      "epoch 13 reward mean: 20.0\n",
      "epoch 14 reward mean: 20.0\n",
      "epoch 15 reward mean: 20.0\n",
      "epoch 16 reward mean: 20.0\n",
      "epoch 17 reward mean: 20.0\n",
      "epoch 18 reward mean: 20.0\n",
      "epoch 19 reward mean: 20.0\n",
      "epoch 20 reward mean: 20.0\n",
      "epoch 21 reward mean: 20.0\n",
      "epoch 22 reward mean: 20.0\n",
      "epoch 23 reward mean: 20.0\n",
      "epoch 24 reward mean: 20.0\n",
      "epoch 25 reward mean: 20.0\n",
      "epoch 26 reward mean: 20.0\n",
      "epoch 27 reward mean: 20.0\n",
      "epoch 28 reward mean: 20.0\n",
      "epoch 29 reward mean: 20.0\n",
      "epoch 30 reward mean: 20.0\n",
      "epoch 31 reward mean: 20.0\n",
      "epoch 32 reward mean: 20.0\n",
      "epoch 33 reward mean: 20.0\n",
      "epoch 34 reward mean: 20.0\n",
      "epoch 35 reward mean: 20.0\n",
      "epoch 36 reward mean: 20.0\n",
      "epoch 37 reward mean: 20.0\n",
      "epoch 38 reward mean: 20.0\n",
      "epoch 39 reward mean: 20.0\n",
      "epoch 40 reward mean: 20.0\n",
      "epoch 41 reward mean: 20.0\n",
      "epoch 42 reward mean: 20.0\n",
      "epoch 43 reward mean: 20.0\n",
      "epoch 44 reward mean: 20.0\n",
      "epoch 45 reward mean: 20.0\n",
      "epoch 46 reward mean: 20.0\n",
      "epoch 47 reward mean: 20.0\n",
      "epoch 48 reward mean: 20.0\n",
      "epoch 49 reward mean: 20.0\n"
     ]
    }
   ],
   "source": [
    "# Testando o histórico de mudanças\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    train_fit(qll, env, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "**********************************\nResultados\n**********************************\nMédia de ações por episódio: 5000.0\nPenalidade média por episódio: 838.6\n"
     ]
    }
   ],
   "source": [
    "total_actions, total_penalties = 0, 0\n",
    "NUM_EPISODES = 10\n",
    "\n",
    "for e in range(NUM_EPISODES):\n",
    "\n",
    "    state = env.reset()\n",
    "    actions = 0\n",
    "    penalties = 0\n",
    "    reward = 0\n",
    "    \n",
    "    count = 1000\n",
    "\n",
    "    while True and count > 0:\n",
    "        action = qll.choose_action(state, explore = False)\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        actions += 1\n",
    "\n",
    "        if done:\n",
    "          break\n",
    "\n",
    "        count -=1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_actions += actions\n",
    "\n",
    "print(\"**********************************\")\n",
    "print(\"Resultados\")\n",
    "print(\"**********************************\")\n",
    "print(\"Média de ações por episódio: {}\".format(total_actions / NUM_EPISODES))\n",
    "print(\"Penalidade média por episódio: {}\".format(total_penalties / NUM_EPISODES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}